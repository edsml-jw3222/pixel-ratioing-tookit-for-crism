{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, GroupKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_scalers(model, df_data, cv_splits=5, random_seed=42):\n",
    "    \"\"\"\n",
    "    Evaluate a given model using different scalers.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to evaluate.\n",
    "    - df_data: The dataframe containing the data.\n",
    "    - cv_splits: Number of splits for GroupKFold cross-validation. Default is 20.\n",
    "    - random_seed: Random seed for shuffling. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    - None. Prints out the evaluation metrics for each scaler.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = df_data.columns[3:-1]\n",
    "    label = df_data.columns[-1]\n",
    "\n",
    "    scalers = {\n",
    "        \"StandardScaler\": StandardScaler(),\n",
    "        \"MinMaxScaler\": MinMaxScaler(),\n",
    "        \"RobustScaler\": RobustScaler()\n",
    "    }\n",
    "\n",
    "    cv = GroupKFold(n_splits=cv_splits)\n",
    "\n",
    "    for name, scaler in scalers.items():\n",
    "\n",
    "        X_data = df_data[features].values\n",
    "        y_data = df_data[label].values\n",
    "        groups = df_data[\"image_id\"].values\n",
    "\n",
    "        X_scaled = scaler.fit_transform(X_data)\n",
    "\n",
    "        X_scaled, y_data, groups = shuffle(X_scaled, y_data, groups, random_state=random_seed)\n",
    "\n",
    "        scores = cross_validate(model, X_scaled, y_data, groups=groups, cv=cv, \n",
    "                                scoring=['precision', 'recall', 'f1', 'accuracy'], n_jobs=-1)\n",
    "\n",
    "        avg_precision = np.mean(scores['test_precision'])\n",
    "        avg_recall = np.mean(scores['test_recall'])\n",
    "        avg_fscore = np.mean(scores['test_f1'])\n",
    "        avg_accuracy = np.mean(scores['test_accuracy'])\n",
    "\n",
    "        print(f\"Using {name}\")\n",
    "        print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "        print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "        print(f\"Average F1-Score: {avg_fscore:.4f}\")\n",
    "        print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
    "        print(\"----------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_pickle('df_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 46\n",
      "Number of missing values: 0\n"
     ]
    }
   ],
   "source": [
    "feature_columns = list(range(0, 438))\n",
    "duplicates = df_data.duplicated(subset=feature_columns, keep=False)\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f'Number of duplicate rows: {num_duplicates}')\n",
    "\n",
    "missing_count = df_data.isnull().sum().sum()\n",
    "print(f'Number of missing values: {missing_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "df_data = df_data[~duplicates]\n",
    "duplicates = df_data.duplicated(subset=feature_columns, keep=False)\n",
    "num_duplicates = duplicates.sum()\n",
    "print(f'Number of duplicate rows: {num_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using StandardScaler\n",
      "Average Precision: 0.8120\n",
      "Average Recall: 0.9056\n",
      "Average F1-Score: 0.8560\n",
      "Average Accuracy: 0.8917\n",
      "----------------------------\n",
      "Using MinMaxScaler\n",
      "Average Precision: 0.8121\n",
      "Average Recall: 0.8889\n",
      "Average F1-Score: 0.8484\n",
      "Average Accuracy: 0.8869\n",
      "----------------------------\n",
      "Using RobustScaler\n",
      "Average Precision: 0.8123\n",
      "Average Recall: 0.9053\n",
      "Average F1-Score: 0.8560\n",
      "Average Accuracy: 0.8918\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(solver='newton-cholesky', max_iter=200, n_jobs=-1)\n",
    "evaluate_with_scalers(lr, df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using StandardScaler\n",
      "Average Precision: 0.5260\n",
      "Average Recall: 0.9685\n",
      "Average F1-Score: 0.6625\n",
      "Average Accuracy: 0.6255\n",
      "----------------------------\n",
      "Using MinMaxScaler\n",
      "Average Precision: 0.5215\n",
      "Average Recall: 0.9689\n",
      "Average F1-Score: 0.6597\n",
      "Average Accuracy: 0.6223\n",
      "----------------------------\n",
      "Using RobustScaler\n",
      "Average Precision: 0.5218\n",
      "Average Recall: 0.9680\n",
      "Average F1-Score: 0.6594\n",
      "Average Accuracy: 0.6212\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "evaluate_with_scalers(rf, df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using StandardScaler\n",
      "Average Precision: 0.5680\n",
      "Average Recall: 0.8559\n",
      "Average F1-Score: 0.6658\n",
      "Average Accuracy: 0.6933\n",
      "----------------------------\n",
      "Using MinMaxScaler\n",
      "Average Precision: 0.5779\n",
      "Average Recall: 0.8787\n",
      "Average F1-Score: 0.6778\n",
      "Average Accuracy: 0.6979\n",
      "----------------------------\n",
      "Using RobustScaler\n",
      "Average Precision: 0.5524\n",
      "Average Recall: 0.8497\n",
      "Average F1-Score: 0.6528\n",
      "Average Accuracy: 0.6778\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5, n_jobs=-1)\n",
    "evaluate_with_scalers(knn, df_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
